一套**可直接跑**的最小完整示例项目（**JDK 17 + Spring Boot 3.5.6 + WebFlux + Parquet 1.16.0 + Hadoop 3.4.1**），满足：

* **不再使用 `Flux<Map<...>>`**，不做“每行一个 Map”
* **Parquet → CSV**：**读一行写一行**（不落地 CSV，不攒内存）
* **Parquet → ZIP(CSV)**：**边生成 CSV 边压缩写出**（ZipOutputStream 流式写 entry）
* **null 输出为空**
* **BINARY 输出“原始 bytes”**：本示例将 CSV **编码为 ISO-8859-1**，对 BINARY 做 `new String(bytes, ISO_8859_1)` 再写出，确保“写到 CSV 的字节序列”与原始 bytes 一致（注意：CSV 本质是文本格式；想要严格字节级可逆，必须约束编码/解析方式）

并且用 `fileFormat` 枚举控制输出类型（PARQUET / CSV / ZIP）。

> 技术要点你之前认可过的那套：
>
> * WebFlux 的背压来自 Reactive Streams 的“**按需请求**（request-n）”，目标是避免下游慢导致上游无限缓冲从而 OOM ([GitHub][1])
> * `DataBufferUtils.outputStreamPublisher(...)` 可以把“写 OutputStream 的阻塞式代码”安全桥接成 `Publisher<DataBuffer>`（用于 HTTP 响应体流式输出）([Home][2])

---

## 1) 项目结构

```
webflux-parquet-export-demo/
  pom.xml
  src/main/java/com/example/demo/
    DemoApplication.java
    config/ExportConfig.java
    config/WebConfig.java
    controller/DemoController.java
    model/FileFormat.java
    service/ParquetExportService.java
    util/CsvUtil.java
    util/Int96Util.java
```

---

## 2) pom.xml（Maven）

> 说明：Boot 3.5.6 作为 parent；Parquet/Hadoop 按你指定版本显式引入。

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <groupId>com.example</groupId>
  <artifactId>webflux-parquet-export-demo</artifactId>
  <version>1.0.0</version>
  <name>webflux-parquet-export-demo</name>

  <parent>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-parent</artifactId>
    <version>3.5.6</version>
    <relativePath/>
  </parent>

  <properties>
    <java.version>17</java.version>
    <parquet.version>1.16.0</parquet.version>
    <hadoop.version>3.4.1</hadoop.version>
  </properties>

  <dependencies>
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter-webflux</artifactId>
    </dependency>

    <!-- Parquet -->
    <dependency>
      <groupId>org.apache.parquet</groupId>
      <artifactId>parquet-hadoop</artifactId>
      <version>${parquet.version}</version>
    </dependency>

    <!-- Hadoop (local fs + Path + Configuration, etc.) -->
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-common</artifactId>
      <version>${hadoop.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-mapreduce-client-core</artifactId>
      <version>${hadoop.version}</version>
    </dependency>

    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter-test</artifactId>
      <scope>test</scope>
      <exclusions>
        <exclusion>
          <groupId>org.junit.vintage</groupId>
          <artifactId>junit-vintage-engine</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
    <dependency>
      <groupId>io.projectreactor</groupId>
      <artifactId>reactor-test</artifactId>
      <scope>test</scope>
    </dependency>
  </dependencies>

  <build>
    <plugins>
      <plugin>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-maven-plugin</artifactId>
      </plugin>
    </plugins>
  </build>
</project>
```

---

## 3) 代码

### 3.1 DemoApplication.java

```java
package com.example.demo;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class DemoApplication {
  public static void main(String[] args) {
    SpringApplication.run(DemoApplication.class, args);
  }
}
```

---

### 3.2 model/FileFormat.java（枚举）

```java
package com.example.demo.model;

import java.util.Locale;

public enum FileFormat {
  PARQUET,
  CSV,
  ZIP;

  public static FileFormat from(String raw) {
    if (raw == null) return ZIP;
    return FileFormat.valueOf(raw.trim().toUpperCase(Locale.ROOT));
  }
}
```

---

### 3.3 config/WebConfig.java（让 query param 支持大小写）

```java
package com.example.demo.config;

import com.example.demo.model.FileFormat;
import org.springframework.core.convert.converter.Converter;
import org.springframework.format.FormatterRegistry;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.config.WebFluxConfigurer;

@Component
public class WebConfig implements WebFluxConfigurer {

  @Override
  public void addFormatters(FormatterRegistry registry) {
    registry.addConverter(new Converter<String, FileFormat>() {
      @Override
      public FileFormat convert(String source) {
        return FileFormat.from(source);
      }
    });
  }
}
```

---

### 3.4 config/ExportConfig.java（线程池 + 限制项）

```java
package com.example.demo.config;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

import java.util.concurrent.*;

@Configuration
public class ExportConfig {

  // 你可按机器核数/并发量调整
  @Bean(destroyMethod = "shutdown")
  public ExecutorService exportExecutor() {
    int cores = Math.max(2, Runtime.getRuntime().availableProcessors());
    int poolSize = Math.min(8, cores);

    return new ThreadPoolExecutor(
        poolSize,
        poolSize,
        60L,
        TimeUnit.SECONDS,
        // 有界队列：避免任务无限堆积导致内存膨胀
        new ArrayBlockingQueue<>(64),
        new ThreadFactory() {
          private final ThreadFactory df = Executors.defaultThreadFactory();
          @Override public Thread newThread(Runnable r) {
            Thread t = df.newThread(r);
            t.setName("export-io-" + t.getId());
            t.setDaemon(true);
            return t;
          }
        },
        // 队列满时，回退到调用线程执行（简单兜底策略）
        new ThreadPoolExecutor.CallerRunsPolicy()
    );
  }

  // 流式输出切片大小（DataBuffer chunk）
  @Bean
  public Integer exportChunkSize() {
    return 64 * 1024; // 64KB
  }

  // 示例：最多导出多少行（避免“2GB parquet -> 无穷大 CSV”）
  @Bean
  public Long maxExportRows() {
    return 5_000_000L;
  }
}
```

---

### 3.5 controller/DemoController.java

```java
package com.example.demo.controller;

import com.example.demo.model.FileFormat;
import com.example.demo.service.ParquetExportService;
import org.springframework.core.io.buffer.DataBuffer;
import org.springframework.http.HttpHeaders;
import org.springframework.http.MediaType;
import org.springframework.http.server.reactive.ServerHttpResponse;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.core.scheduler.Schedulers;

import java.nio.file.Path;
import java.util.LinkedHashMap;
import java.util.Map;

@RestController
@RequestMapping("/demo")
public class DemoController {

  private final ParquetExportService service;
  private final Long maxExportRows;
  private final Integer chunkSize;

  public DemoController(ParquetExportService service, Long maxExportRows, Integer chunkSize) {
    this.service = service;
    this.maxExportRows = maxExportRows;
    this.chunkSize = chunkSize;
  }

  /**
   * 生成一个本地 Parquet 文件（写到项目当前路径 ./data/demo.parquet）
   * 用于你在 Mac/Windows 上快速压测。
   *
   * 示例：
   *   POST http://localhost:8080/demo/generate?rows=800000
   */
  @PostMapping("/generate")
  public Mono<Map<String, Object>> generate(@RequestParam(defaultValue = "500000") long rows) {
    return Mono.fromCallable(() -> service.generateDemoParquet(rows))
        .subscribeOn(Schedulers.boundedElastic())
        .map(path -> {
          Map<String, Object> resp = new LinkedHashMap<>();
          resp.put("parquetPath", path.toAbsolutePath().toString());
          resp.put("rows", rows);
          resp.put("sizeBytes", service.size(path));
          return resp;
        });
  }

  /**
   * 下载接口：
   *   GET /demo/download?format=zip|csv|parquet
   */
  @GetMapping("/download")
  public Mono<Void> download(@RequestParam(defaultValue = "zip") FileFormat format,
                             ServerHttpResponse response) {

    Path parquet = service.getDemoParquetOrThrow();

    // headers
    response.getHeaders().set(HttpHeaders.CACHE_CONTROL, "no-store");
    response.getHeaders().set(HttpHeaders.PRAGMA, "no-cache");
    response.getHeaders().set("X-Content-Type-Options", "nosniff");

    String filename;
    MediaType contentType;

    switch (format) {
      case PARQUET -> {
        filename = "data.parquet";
        contentType = MediaType.APPLICATION_OCTET_STREAM;
      }
      case CSV -> {
        filename = "data.csv";
        contentType = new MediaType("text", "csv");
      }
      case ZIP -> {
        filename = "data.zip";
        contentType = new MediaType("application", "zip");
      }
      default -> throw new IllegalArgumentException("Unsupported format: " + format);
    }

    response.getHeaders().setContentType(contentType);
    response.getHeaders().set(HttpHeaders.CONTENT_DISPOSITION,
        "attachment; filename=\"" + filename + "\"");

    Flux<DataBuffer> body = service.export(parquet, format, response.bufferFactory(), chunkSize, maxExportRows);

    // writeWith 本身就是背压友好的：客户端慢 -> 下游 demand 小 -> 上游产出也会被抑制 
    return response.writeWith(body);
  }
}
```

---

### 3.6 service/ParquetExportService.java（核心：Parquet→CSV/ZIP 流式写出）

```java
package com.example.demo.service;

import com.example.demo.model.FileFormat;
import com.example.demo.util.CsvUtil;
import com.example.demo.util.Int96Util;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.example.data.Group;
import org.apache.parquet.example.data.simple.SimpleGroupFactory;
import org.apache.parquet.hadoop.ParquetFileReader;
import org.apache.parquet.hadoop.ParquetReader;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.example.GroupReadSupport;
import org.apache.parquet.hadoop.example.GroupWriteSupport;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;
import org.apache.parquet.hadoop.util.HadoopInputFile;
import org.apache.parquet.io.api.Binary;
import org.apache.parquet.schema.MessageType;
import org.apache.parquet.schema.MessageTypeParser;
import org.apache.parquet.schema.Type;
import org.apache.parquet.schema.PrimitiveType;
import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName;
import org.springframework.core.io.buffer.DataBuffer;
import org.springframework.core.io.buffer.DataBufferFactory;
import org.springframework.core.io.buffer.DataBufferUtils;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;

import java.io.BufferedOutputStream;
import java.io.IOException;
import java.io.OutputStream;
import java.nio.charset.Charset;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.time.Instant;
import java.util.Random;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.atomic.AtomicReference;
import java.util.zip.Deflater;
import java.util.zip.ZipEntry;
import java.util.zip.ZipOutputStream;

@Service
public class ParquetExportService {

  /**
   * 为了“BINARY 输出原始 bytes”，本示例 CSV 使用 ISO-8859-1 编码输出：
   * bytes(0-255) <-> char(0-255) 一一映射，写出字节不变。
   */
  public static final Charset CSV_CHARSET = Charset.forName("ISO-8859-1");

  private static final String DEMO_SCHEMA = """
      message demo {
        optional int32   i32;
        optional int64   i64;
        optional int96   t96;
        optional float   f32;
        optional boolean b;
        optional double  d64;
        optional binary  bin;
      }
      """;

  private final ExecutorService exportExecutor;
  private final AtomicReference<java.nio.file.Path> demoParquetPath = new AtomicReference<>();

  public ParquetExportService(ExecutorService exportExecutor) {
    this.exportExecutor = exportExecutor;
  }

  public long size(java.nio.file.Path p) {
    try {
      return Files.size(p);
    } catch (IOException e) {
      throw new RuntimeException(e);
    }
  }

  public java.nio.file.Path getDemoParquetOrThrow() {
    java.nio.file.Path p = demoParquetPath.get();
    if (p == null || !Files.exists(p)) {
      throw new IllegalStateException("Demo parquet not found. Call POST /demo/generate first.");
    }
    return p;
  }

  /**
   * 在 ./data/demo.parquet 生成一个可用于压测的 Parquet。
   */
  public java.nio.file.Path generateDemoParquet(long rows) throws IOException {
    java.nio.file.Path dir = Paths.get(System.getProperty("user.dir"), "data");
    Files.createDirectories(dir);
    java.nio.file.Path out = dir.resolve("demo.parquet");

    MessageType schema = MessageTypeParser.parseMessageType(DEMO_SCHEMA);
    Configuration conf = new Configuration();
    GroupWriteSupport.setSchema(schema, conf);

    SimpleGroupFactory factory = new SimpleGroupFactory(schema);
    Random r = new Random(1234567L);

    // ParquetWriter<Group>（压缩建议用 SNAPPY；也可按需改 GZIP/ZSTD）
    try (ParquetWriter<Group> writer =
             new ParquetWriter<>(
                 new Path(out.toUri()),
                 new GroupWriteSupport(),
                 CompressionCodecName.SNAPPY,
                 ParquetWriter.DEFAULT_BLOCK_SIZE,
                 ParquetWriter.DEFAULT_PAGE_SIZE,
                 ParquetWriter.DEFAULT_PAGE_SIZE,
                 ParquetWriter.DEFAULT_IS_DICTIONARY_ENABLED,
                 ParquetWriter.DEFAULT_IS_VALIDATING_ENABLED,
                 org.apache.parquet.column.ParquetProperties.WriterVersion.PARQUET_1_0,
                 conf
             )) {

      for (long i = 0; i < rows; i++) {
        Group g = factory.newGroup();

        // 让 optional 列随机为空，测试 “null -> empty”
        if (r.nextInt(10) != 0) g.add("i32", r.nextInt());
        if (r.nextInt(10) != 0) g.add("i64", r.nextLong());
        if (r.nextInt(10) != 0) {
          Instant now = Instant.ofEpochMilli(System.currentTimeMillis() + r.nextInt(1_000_000));
          byte[] int96 = Int96Util.instantToInt96(now);
          g.add("t96", Binary.fromConstantByteArray(int96));
        }
        if (r.nextInt(10) != 0) g.add("f32", r.nextFloat());
        if (r.nextInt(10) != 0) g.add("b", r.nextBoolean());
        if (r.nextInt(10) != 0) g.add("d64", r.nextDouble());

        if (r.nextInt(10) != 0) {
          // 生成“可见 ASCII”字节（便于你肉眼看 CSV，不代表真实二进制场景）
          byte[] bytes = new byte[16];
          for (int k = 0; k < bytes.length; k++) {
            int ch = 33 + r.nextInt(94); // '!'..'~'
            bytes[k] = (byte) ch;
          }
          g.add("bin", Binary.fromConstantByteArray(bytes));
        }

        writer.write(g);
      }
    }

    demoParquetPath.set(out);
    return out;
  }

  /**
   * 导出：PARQUET 原样 / CSV / ZIP(CSV)
   */
  public Flux<DataBuffer> export(java.nio.file.Path parquetFile,
                                 FileFormat format,
                                 DataBufferFactory bufferFactory,
                                 int chunkSize,
                                 long maxRows) {

    return switch (format) {
      case PARQUET -> DataBufferUtils.read(parquetFile, bufferFactory, 64 * 1024);

      case CSV -> Flux.from(
          DataBufferUtils.outputStreamPublisher(
              os -> writeCsvTo(os, parquetFile, maxRows),
              bufferFactory,
              exportExecutor,
              chunkSize
          )
      );

      case ZIP -> Flux.from(
          DataBufferUtils.outputStreamPublisher(
              os -> writeZipCsvTo(os, parquetFile, maxRows),
              bufferFactory,
              exportExecutor,
              chunkSize
          )
      );
    };
  }

  private void writeZipCsvTo(OutputStream rawOut, java.nio.file.Path parquetFile, long maxRows) {
    try {
      // 额外包一层 buffer，减少系统调用
      BufferedOutputStream bos = new BufferedOutputStream(rawOut, 64 * 1024);

      try (ZipOutputStream zos = new ZipOutputStream(bos, CSV_CHARSET)) {
        zos.setLevel(Deflater.BEST_SPEED);

        ZipEntry entry = new ZipEntry("data.csv");
        zos.putNextEntry(entry);

        // 关键：直接把 CSV 写到 ZipEntry 的输出流里 -> 边写边压缩
        writeCsvTo(zos, parquetFile, maxRows);

        zos.closeEntry();
        zos.finish();
        zos.flush();
      }
    } catch (IOException e) {
      // 客户端取消/断开时常见 "Broken pipe"：不再继续写，避免无意义堆栈刷屏
      if (!isClientAbort(e)) {
        throw new RuntimeException(e);
      }
    }
  }

  private void writeCsvTo(OutputStream out, java.nio.file.Path parquetFile, long maxRows) {
    Configuration conf = new Configuration();
    Path hPath = new Path(parquetFile.toUri());

    try {
      MessageType schema = readSchema(conf, hPath);

      // 1) header
      CsvUtil.writeHeader(out, schema, CSV_CHARSET);

      // 2) rows：ParquetReader<Group> 一行行读，一行行写
      GroupReadSupport readSupport = new GroupReadSupport();
      try (ParquetReader<Group> reader = ParquetReader.builder(readSupport, hPath).withConf(conf).build()) {
        long row = 0;
        Group g;
        while ((g = reader.read()) != null) {
          row++;
          if (row > maxRows) break;

          CsvUtil.writeRow(out, g, schema, CSV_CHARSET, this::cellValueToStringOrBytes);
        }
      }

      out.flush();
    } catch (IOException e) {
      if (!isClientAbort(e)) {
        throw new RuntimeException(e);
      }
    }
  }

  private MessageType readSchema(Configuration conf, Path hPath) throws IOException {
    try (ParquetFileReader pfr = ParquetFileReader.open(HadoopInputFile.fromPath(hPath, conf))) {
      return pfr.getFileMetaData().getSchema();
    }
  }

  /**
   * 将 Group 的 cell 转成 String（null->null），BINARY 输出“原始 bytes（ISO-8859-1）”。
   */
  private CsvUtil.Cell cellValueToStringOrBytes(Group g, int fieldIndex, Type fieldType) {
    if (g.getFieldRepetitionCount(fieldIndex) == 0) {
      return CsvUtil.Cell.empty(); // null -> empty
    }

    if (!fieldType.isPrimitive()) {
      // demo 只处理 primitive；真实项目可以在这里扩展 group/list/map 的展开策略
      return CsvUtil.Cell.ofString(g.getGroup(fieldIndex, 0).toString());
    }

    PrimitiveType pt = fieldType.asPrimitiveType();
    PrimitiveTypeName ptn = pt.getPrimitiveTypeName();

    return switch (ptn) {
      case INT32 -> CsvUtil.Cell.ofString(Integer.toString(g.getInteger(fieldIndex, 0)));
      case INT64 -> CsvUtil.Cell.ofString(Long.toString(g.getLong(fieldIndex, 0)));
      case FLOAT -> CsvUtil.Cell.ofString(Float.toString(g.getFloat(fieldIndex, 0)));
      case DOUBLE -> CsvUtil.Cell.ofString(Double.toString(g.getDouble(fieldIndex, 0)));
      case BOOLEAN -> CsvUtil.Cell.ofString(Boolean.toString(g.getBoolean(fieldIndex, 0)));

      case INT96 -> {
        // INT96 常用于 timestamp：这里转 ISO-8601；如果你想“原始 bytes”，改成 ofBytes(...)
        Binary b = g.getInt96(fieldIndex, 0);
        Instant instant = Int96Util.int96ToInstant(b.getBytes());
        yield CsvUtil.Cell.ofString(instant.toString());
      }

      case BINARY, FIXED_LEN_BYTE_ARRAY -> {
        Binary b = g.getBinary(fieldIndex, 0);
        // 原始 bytes：用 ISO-8859-1 做一一映射
        yield CsvUtil.Cell.ofBytes(b.getBytes());
      }

      default -> CsvUtil.Cell.ofString(g.getValueToString(fieldIndex, 0));
    };
  }

  private boolean isClientAbort(IOException e) {
    String msg = (e.getMessage() == null) ? "" : e.getMessage().toLowerCase();
    return msg.contains("broken pipe") || msg.contains("connection reset") || msg.contains("forcibly closed");
  }
}
```

---

### 3.7 util/CsvUtil.java（**不建 Map、不攒行集合**，直接写 OutputStream）

```java
package com.example.demo.util;

import org.apache.parquet.example.data.Group;
import org.apache.parquet.schema.MessageType;
import org.apache.parquet.schema.Type;

import java.io.IOException;
import java.io.OutputStream;
import java.nio.charset.Charset;
import java.util.function.Function;

public final class CsvUtil {

  private CsvUtil() {}

  /**
   * 单元格：要么是 String（文本），要么是 bytes（按 charset 一一映射输出）。
   */
  public sealed interface Cell permits CellString, CellBytes, CellEmpty {
    static Cell ofString(String s) { return new CellString(s); }
    static Cell ofBytes(byte[] b) { return new CellBytes(b); }
    static Cell empty() { return new CellEmpty(); }
  }

  public record CellString(String value) implements Cell {}
  public record CellBytes(byte[] bytes) implements Cell {}
  public record CellEmpty() implements Cell {}

  @FunctionalInterface
  public interface CellExtractor {
    Cell extract(Group g, int fieldIndex, Type fieldType);
  }

  public static void writeHeader(OutputStream out, MessageType schema, Charset charset) throws IOException {
    int n = schema.getFieldCount();
    for (int i = 0; i < n; i++) {
      if (i > 0) out.write(',');
      writeCsvEscapedString(out, schema.getFieldName(i), charset);
    }
    out.write('\n');
  }

  public static void writeRow(OutputStream out,
                              Group g,
                              MessageType schema,
                              Charset charset,
                              CellExtractor extractor) throws IOException {

    int n = schema.getFieldCount();
    for (int i = 0; i < n; i++) {
      if (i > 0) out.write(',');

      Type t = schema.getType(i);
      Cell cell = extractor.extract(g, i, t);

      if (cell instanceof CellEmpty) {
        // null -> empty (什么都不写)
        continue;
      } else if (cell instanceof CellString cs) {
        writeCsvEscapedString(out, cs.value(), charset);
      } else if (cell instanceof CellBytes cb) {
        // “原始 bytes”：按 charset 输出；为避免破坏 CSV 结构，仍按 CSV 规则决定是否加引号/转义
        writeCsvEscapedBytes(out, cb.bytes(), charset);
      }
    }
    out.write('\n');
  }

  /**
   * RFC4180 风格的最小转义：包含逗号/引号/换行 -> 用双引号包起来，引号本身变成两个引号。
   */
  private static void writeCsvEscapedString(OutputStream out, String s, Charset charset) throws IOException {
    if (s == null) return;
    byte[] bytes = s.getBytes(charset);
    writeCsvEscapedBytes(out, bytes, charset);
  }

  private static void writeCsvEscapedBytes(OutputStream out, byte[] bytes, Charset charset) throws IOException {
    if (bytes == null || bytes.length == 0) return;

    boolean needQuote = false;
    for (byte b : bytes) {
      if (b == ',' || b == '"' || b == '\n' || b == '\r') {
        needQuote = true;
        break;
      }
    }

    if (!needQuote) {
      out.write(bytes);
      return;
    }

    out.write('"');
    for (byte b : bytes) {
      if (b == '"') {
        // " -> ""
        out.write('"');
        out.write('"');
      } else {
        out.write(b);
      }
    }
    out.write('"');
  }
}
```

---

### 3.8 util/Int96Util.java（INT96 <-> Instant）

```java
package com.example.demo.util;

import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.time.Instant;

public final class Int96Util {

  private Int96Util() {}

  // Julian day for 1970-01-01
  private static final int JULIAN_DAY_OF_EPOCH = 2440588;
  private static final long NANOS_PER_DAY = 86_400_000_000_000L;
  private static final long NANOS_PER_SECOND = 1_000_000_000L;

  /**
   * Parquet INT96: little-endian [nanosOfDay(8 bytes), julianDay(4 bytes)]
   */
  public static Instant int96ToInstant(byte[] int96) {
    if (int96 == null || int96.length != 12) {
      throw new IllegalArgumentException("INT96 must be 12 bytes");
    }
    ByteBuffer bb = ByteBuffer.wrap(int96).order(ByteOrder.LITTLE_ENDIAN);
    long nanosOfDay = bb.getLong();
    int julianDay = bb.getInt();

    long epochDay = (long) julianDay - JULIAN_DAY_OF_EPOCH;
    long epochSecond = epochDay * 86_400L + (nanosOfDay / NANOS_PER_SECOND);
    long nanoAdj = nanosOfDay % NANOS_PER_SECOND;
    return Instant.ofEpochSecond(epochSecond, nanoAdj);
  }

  public static byte[] instantToInt96(Instant instant) {
    long epochSecond = instant.getEpochSecond();
    int nano = instant.getNano();

    long epochDay = Math.floorDiv(epochSecond, 86_400L);
    long secondsOfDay = Math.floorMod(epochSecond, 86_400L);

    int julianDay = (int) (epochDay + JULIAN_DAY_OF_EPOCH);
    long nanosOfDay = secondsOfDay * NANOS_PER_SECOND + nano;

    ByteBuffer bb = ByteBuffer.allocate(12).order(ByteOrder.LITTLE_ENDIAN);
    bb.putLong(nanosOfDay);
    bb.putInt(julianDay);
    return bb.array();
  }
}
```

---

## 4) 怎么跑

1. `mvn -q -DskipTests package`
2. `mvn spring-boot:run`
3. 生成 Parquet（你可以调 rows 来接近 50MB）
   `POST http://localhost:8080/demo/generate?rows=800000`
4. 下载：

    * `GET http://localhost:8080/demo/download?format=parquet`
    * `GET http://localhost:8080/demo/download?format=csv`
    * `GET http://localhost:8080/demo/download?format=zip`

---

## 5) 为什么这套不会 OOM（针对你们当前痛点的“落地解释”）

* **不再用 `Flux.create + sink.next(map)`**：这种写法天然偏“push”，一旦下游（HTTP 写出）慢，很容易把对象（Map/行缓冲）堆在内存里；背压的目标就是避免“下游慢 -> 上游无限产出 -> 无界缓冲 OOM”
* **改为 `outputStreamPublisher`**：把“阻塞式读 Parquet + 写 CSV/ZIP”的逻辑放到专用线程池里执行，并通过 `Publisher<DataBuffer>` 交给 WebFlux 写响应体；这是 Spring 在 DataBuffer 层提供的桥接能力
* **ZIP 可以边写边压缩**：`ZipOutputStream` 写 entry 时就是“流式 deflate”，你不需要先拿到完整 CSV 文件，更不需要把 CSV 缓存在内存里；因此可以做到“读一行 -> 写一行 -> 同时被压缩后发给客户端”。

---

如果你希望把这套示例进一步贴近生产（比如：支持投影列、支持多 sheet/多 entry、支持统计导出行数/耗时、支持超时与主动取消、支持复杂 Parquet schema 展开策略），我建议你先以这套跑通压测，再逐项加能力。

[1]: https://github.com/reactive-streams/reactive-streams-jvm "GitHub - reactive-streams/reactive-streams-jvm: Reactive Streams Specification for the JVM"
[2]: https://docs.spring.io/spring-framework/docs/6.1.21/javadoc-api/org/springframework/core/io/buffer/DataBufferUtils.html "DataBufferUtils (Spring Framework 6.1.21 API)"
